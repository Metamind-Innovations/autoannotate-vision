# AutoAnnotate-Vision ğŸ¯

**State-of-the-art unsupervised auto-annotation SDK for image classification with GUI**

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

AutoAnnotate-Vision automatically clusters and organizes unlabeled image datasets using cutting-edge vision models (CLIP, DINOv2). Features a **graphical user interface** for easy use and **HTML preview** for visual cluster inspection.

## âœ¨ Features

- ğŸ¨ **Graphical User Interface**: Easy folder browsers and visual controls
- ğŸ–¼ï¸ **HTML Image Preview**: View cluster samples in browser before labeling
- ğŸ¤– **SOTA Vision Models**: CLIP, DINOv2, DINOv2-Large
- ğŸ”¬ **Multiple Clustering**: K-means, Spectral, DBSCAN
- ğŸ“ **Smart Organization**: Preserves original filenames
- âœ‚ï¸ **Auto Splits**: Train/val/test dataset splitting
- ğŸ’¾ **Export**: CSV, JSON formats
- ğŸ”Œ **Python API**: Full programmatic control

## ğŸš€ Installation

```bash
pip install autoannotate-vision
```

Or from source:
```bash
git clone https://github.com/Metamind-Innovations/autoannotate-vision.git
cd autoannotate-vision
pip install -e .
```

## ğŸ¨ Quick Start - GUI

The easiest way to use AutoAnnotate-Vision:

```bash
python run_autoannotate_gui.py
```

**Workflow:**
1. ğŸ“ Select input folder with images
2. ğŸ“‚ Select output folder  
3. ğŸ”¢ Set number of classes
4. ğŸ¤– Choose model (dinov2 recommended)
5. â–¶ï¸ Click "Start Auto-Annotation"

The app will cluster images and open **HTML previews** in your browser showing sample images from each cluster for easy labeling!

## ğŸ’» CLI Usage

```bash
autoannotate annotate /path/to/images /path/to/output \
    --n-clusters 10 \
    --method kmeans \
    --model dinov2 \
    --create-splits
```

## ğŸ Python API

```python
from autoannotate import AutoAnnotator

annotator = AutoAnnotator(
    input_dir="./images",
    output_dir="./output",
    model="dinov2",
    clustering_method="kmeans",
    n_clusters=5
)

result = annotator.run_full_pipeline(create_splits=True)
print(f"Processed {result['n_images']} images")
```

## ğŸ“ Output Structure

```
output/
â”œâ”€â”€ metadata.json
â”œâ”€â”€ labels.csv
â”œâ”€â”€ cats/              # Your class names
â”‚   â”œâ”€â”€ IMG_001.jpg   # Original filenames preserved!
â”‚   â””â”€â”€ ...
â”œâ”€â”€ dogs/
â””â”€â”€ splits/            # train/val/test
    â”œâ”€â”€ train/
    â”œâ”€â”€ val/
    â””â”€â”€ test/
```

## ğŸ§  Model Comparison

| Model | Speed | Quality | Best For |
|-------|-------|---------|----------|
| CLIP | âš¡âš¡ | â­â­â­ | General images |
| DINOv2 | âš¡âš¡âš¡ | â­â­â­â­ | Recommended |
| DINOv2-Large | âš¡ | â­â­â­â­â­ | High-quality |

## ğŸ” Pre-Push Checklist

Before pushing code:

```bash
# Format code
black src/autoannotate tests

# Run tests
pytest tests/ -v

# Check everything at once
black --check src/autoannotate tests && pytest tests/ -v
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch
3. **Format with Black**: `black src/autoannotate tests`
4. **Run tests**: `pytest tests/ -v`
5. Push and create PR

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file.

## ğŸ™ Acknowledgments

Built with PyTorch, Transformers, scikit-learn. Vision models: CLIP, DINOv2.

**Made for the [RAIDO Project](https://raido-project.eu/)**